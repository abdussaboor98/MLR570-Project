{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: Train: (197944, 22), Test: (49487, 22)\n",
      "Preprocessing\n",
      "Encoding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\preprocessing\\_encoders.py:242: UserWarning: Found unknown categories in columns [4, 5, 6, 7] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Categorical, Real\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.amp import GradScaler, autocast\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "flight_data_train = pd.read_csv(r'D:\\MasterUniversity\\AdvancedML\\Project\\DubaiData\\flight_data_train_ts_wx.csv')\n",
    "flight_data_test = pd.read_csv(r'D:\\MasterUniversity\\AdvancedML\\Project\\DubaiData\\flight_data_test_ts_wx.csv')\n",
    "\n",
    "print(f'Data shape: Train: {flight_data_train.shape}, Test: {flight_data_test.shape}')\n",
    "\n",
    "print('Preprocessing')\n",
    "flight_data_train['scheduledoffblocktime'] = pd.to_datetime(flight_data_train['scheduledoffblocktime'])\n",
    "flight_data_test['scheduledoffblocktime'] = pd.to_datetime(flight_data_test['scheduledoffblocktime'])\n",
    "\n",
    "flight_data_train.sort_values(by='scheduledoffblocktime', inplace=True)\n",
    "flight_data_test.sort_values(by='scheduledoffblocktime', inplace=True)\n",
    "\n",
    "\n",
    "departdatetime = flight_data_train['scheduledoffblocktime'].dt\n",
    "\n",
    "flight_data_train['depart_day'] = departdatetime.day\n",
    "flight_data_train['depart_month'] = departdatetime.month\n",
    "flight_data_train['depart_dayofweek'] = departdatetime.dayofweek\n",
    "flight_data_train['depart_minute'] = departdatetime.hour * 60 + departdatetime.minute\n",
    "# Test\n",
    "departdatetime = flight_data_test['scheduledoffblocktime'].dt\n",
    "flight_data_test['depart_day'] = departdatetime.day\n",
    "flight_data_test['depart_month'] = departdatetime.month\n",
    "flight_data_test['depart_dayofweek'] = departdatetime.dayofweek\n",
    "flight_data_test['depart_minute'] = departdatetime.hour * 60 + departdatetime.minute\n",
    "\n",
    "flight_data_train.drop(columns=['scheduledoffblocktime'], axis=1, inplace=True)\n",
    "flight_data_test.drop(columns=['scheduledoffblocktime'], axis=1, inplace=True)\n",
    "\n",
    "X_train = flight_data_train.drop(columns=['delay_in_secs', 'finalflightstatus'], axis=1)\n",
    "X_test = flight_data_test.drop(columns=['delay_in_secs', 'finalflightstatus'], axis=1)\n",
    "\n",
    "y_train = flight_data_train['finalflightstatus']\n",
    "y_test = flight_data_test['finalflightstatus']\n",
    "\n",
    "y_train = y_train.map({'On-Time': 0, 'Delayed':1})\n",
    "y_test = y_test.map({'On-Time': 0, 'Delayed':1})\n",
    "\n",
    "print('Encoding')\n",
    "# High cardinality columns - CatBoostEncoder\n",
    "high_cardinality_cols = ['airlinecode_iata', 'destination_iata', 'aircraft_iata', 'publicgatenumber']\n",
    "\n",
    "# One-hot encoding\n",
    "one_hot_column =  ['skyc1', 'skyc2', 'traffictypecode', 'aircraftterminal', 'wxcodes'] + high_cardinality_cols\n",
    "ohe = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "encoded = ohe.fit_transform(X_train[one_hot_column])\n",
    "ohe_new_columns = ohe.get_feature_names_out(one_hot_column)\n",
    "encoded_df = pd.DataFrame(encoded, columns=ohe_new_columns)\n",
    "X_train = pd.concat([X_train.drop(columns=one_hot_column), encoded_df], axis=1)\n",
    "encoded = ohe.transform(X_test[one_hot_column])\n",
    "encoded_df = pd.DataFrame(encoded, columns=ohe_new_columns)\n",
    "X_test = pd.concat([X_test.drop(columns=one_hot_column), encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "# StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Prepare sequences for LSTM\n",
    "def create_sequences(features, target, sequence_length=7):\n",
    "    X, y = [], []\n",
    "    for i in range(len(features) - sequence_length):\n",
    "        X.append(features.iloc[i:i + sequence_length].values)\n",
    "        y.append(target.iloc[i + sequence_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "sequence_length = 7\n",
    "X_train_seq, y_train_seq = create_sequences(X_train, y_train, sequence_length)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test, y_test, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "\u001b[1m1547/1547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 16ms/step - accuracy: 0.7165 - f1_score: 1.2614 - loss: 0.5938 - val_accuracy: 0.6745 - val_f1_score: 3.4952 - val_loss: 0.6302\n",
      "Epoch 2/4\n",
      "\u001b[1m1547/1547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 15ms/step - accuracy: 0.7193 - f1_score: 3.7664 - loss: 0.5807 - val_accuracy: 0.6783 - val_f1_score: 0.6932 - val_loss: 0.6333\n",
      "Epoch 3/4\n",
      "\u001b[1m1547/1547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 14ms/step - accuracy: 0.7212 - f1_score: 5.3360 - loss: 0.5762 - val_accuracy: 0.6787 - val_f1_score: 0.0000e+00 - val_loss: 0.6315\n",
      "Epoch 4/4\n",
      "\u001b[1m1547/1547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 14ms/step - accuracy: 0.7213 - f1_score: 6.9627 - loss: 0.5744 - val_accuracy: 0.6675 - val_f1_score: 10.5505 - val_loss: 0.6332\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "n_features = X_train_seq.shape[2]\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(128, input_shape=(sequence_length, n_features), return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(128, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')  # Single neuron with sigmoid activation for binary classification\n",
    "])\n",
    "\n",
    "from tensorflow.keras.backend import epsilon, round, mean, cast, sum as Ksum\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom F1 score metric for Keras.\n",
    "    \"\"\"\n",
    "    y_true = cast(y_true, 'float32')  # Ensure y_true is float32\n",
    "    y_pred = round(y_pred)  # Convert probabilities to 0 or 1\n",
    "    y_pred = cast(y_pred, 'float32')  # Ensure y_pred is float32\n",
    "    tp = Ksum(y_true * y_pred)  # True positives\n",
    "    precision = tp / (Ksum(y_pred) + epsilon())  # Precision\n",
    "    recall = tp / (Ksum(y_true) + epsilon())  # Recall\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + epsilon())\n",
    "    return f1\n",
    "\n",
    "# Compile the model with F1 score as a metric\n",
    "model.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy', f1_score])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_seq, y_train_seq, validation_data=(X_test_seq, y_test_seq),\n",
    "                    batch_size=128, epochs=4, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1547/1547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.95      0.80     33583\n",
      "           1       0.40      0.07      0.11     15897\n",
      "\n",
      "    accuracy                           0.67     49480\n",
      "   macro avg       0.54      0.51      0.45     49480\n",
      "weighted avg       0.59      0.67      0.58     49480\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test_seq)\n",
    "\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)  # Apply threshold to convert probabilities to class labels\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test_seq, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy: 0.67\n",
    "Precision: 0.40\n",
    "Recall: 0.07\n",
    "F1 Score: 0.11"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
